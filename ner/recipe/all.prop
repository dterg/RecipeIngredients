#location of the training file
trainFileList = data/1.tsv,data/2.tsv,data/3.tsv,data/4.tsv,data/5.tsv,data/6.tsv,data/7.tsv,data/8.tsv,data/9.tsv,data/10.tsv,data/11.tsv,data/12.tsv,data/13.tsv,data/14.tsv,data/15.tsv,data/16.tsv,data/17.tsv,data/18.tsv,data/19.tsv,data/20.tsv,data/21.tsv,data/22.tsv,data/23.tsv,data/24.tsv,data/25.tsv,data/26.tsv,data/27.tsv,data/28.tsv,data/29.tsv,data/30.tsv,data/31.tsv,data/32.tsv,data/33.tsv
#testFile = test.tsv
#location where you would like to save (serialize to) your
#classifier; adding .gz at the end automatically gzips the file,
#making it faster and smaller
serializeTo = ner-model-all.ser.gz

#structure of your training file; this tells the classifier
#that the word is in column 0 and the correct answer is in
#column 1
map = word=0,answer=1

#these are the features we'd like to train with
#some are discussed below, the rest can be
#understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
useNGrams=true
#no ngrams will be included that do not contain either the
#beginning or end of the word
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=-1
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
#the next 4 deal with word shape features
useTypeSeqs=false
useTypeSeqs2=false
useTypeySequences=false
wordShape=none
#terms
normalizeTerms=true
useLemmas=true
usePrevNextLemmas=true
useLemmaAsWord=true
useWordTag=true
#Gazettes
useGazettes=true
gazette=ingredients.gaz.txt
sloppyGazette=true
cleanGazette=true
#Conll
entitySubclassification=IOB2